{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from scipy.linalg import null_space\n",
                "from sklearn.decomposition import PCA\n",
                "np.set_printoptions(precision=4, suppress=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ“˜ Linear Algebra for Data Science\n",
                "\n",
                "Comprehensive notes covering all essential linear algebra concepts required for data science, with detailed explanations, mathematical examples, and Python code."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Scalars, Vectors, and Matrices\n",
                "\n",
                "### ðŸ”¹ Scalars\n",
                "\n",
                "A scalar is a single number, representing magnitude without direction. Scalars can be real or complex numbers and are used to scale vectors or matrices in operations like multiplication.\n",
                "\n",
                "* **Example**: $x = 5$ (a real number), or $z = 3 + 4i$ (a complex number).\n",
                "* **Application**: Scalars are used in data science for scaling features (e.g., normalization) or adjusting model parameters (e.g., learning rate in gradient descent).\n",
                "\n",
                "### ðŸ”¹ Vectors\n",
                "\n",
                "A vector is an ordered array of numbers, representing a point or direction in space. Vectors can be row or column vectors, and their dimension is the number of components.\n",
                "\n",
                "* **Row Vector**: $[1, 2, 3]$\n",
                "* **Column Vector**:\n",
                "  $$\n",
                "  \\begin{bmatrix}\n",
                "  1 \\\\\n",
                "  2 \\\\\n",
                "  3\n",
                "  \\end{bmatrix}\n",
                "  $$\n",
                "* **Geometric Interpretation**: In 2D or 3D, vectors represent points or directions (e.g., a displacement).\n",
                "* **Application**: Vectors represent data points (e.g., feature vectors in machine learning) or model weights."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "v1 = np.array([1, 2, 3])\n",
                "v2 = np.array([4, 5, 6])\n",
                "print(\"Vector Addition:\", v1 + v2)\n",
                "print(\"Scalar Multiplication (2 * v1):\", 2 * v1)\n",
                "print(\"Dot Product:\", np.dot(v1, v2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ”¹ Matrices\n",
                "\n",
                "A matrix is a 2D array of numbers, used to represent linear transformations or datasets. An $m \\times n$ matrix has $m$ rows and $n$ columns.\n",
                "\n",
                "* **Example**:\n",
                "  $$\n",
                "  A = \\begin{bmatrix}1 & 2 \\\\ 3 & 4\\end{bmatrix}\n",
                "  $$\n",
                "* **Application**: Matrices represent datasets (rows as samples, columns as features) or transformations (e.g., rotation, scaling)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "A = np.array([[1, 2], [3, 4]])\n",
                "print(\"Matrix A:\\n\", A)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ”¹ Tensors\n",
                "\n",
                "Tensors generalize scalars (0D), vectors (1D), and matrices (2D) to higher dimensionsA 3D tensor might represent a collection of matrices (e.g., RGB images).\n",
                "\n",
                "* **Example**: A 3D tensor for an RGB image has dimensions (height, width, channels).\n",
                "* **Application**: Tensors are central to deep learning (e.g., TensorFlow, PyTorch)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tensor = np.random.rand(2, 3, 4)\n",
                "print(\"3D Tensor:\\n\", tensor)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Matrix Operations\n",
                "\n",
                "### ðŸ”¹ Addition & Subtraction\n",
                "\n",
                "Matrices of the same size can be added or subtracted element-wise.\n",
                "\n",
                "* **Example**:\n",
                "  $$\n",
                "  A = \\begin{bmatrix}1 & 2 \\\\ 3 & 4\\end{bmatrix},\\quad B = \\begin{bmatrix}5 & 6 \\\\ 7 & 8\\end{bmatrix}\n",
                "  $$\n",
                "  $$\n",
                "  A + B = \\begin{bmatrix}6 & 8 \\\\ 10 & 12\\end{bmatrix}\n",
                "  $$\n",
                "* **Application**: Used in combining datasets or updating weights in neural networks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "A = np.array([[1, 2], [3, 4]])\n",
                "B = np.array([[5, 6], [7, 8]])\n",
                "print(\"A + B:\\n\", A + B)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ”¹ Scalar Multiplication\n",
                "\n",
                "Multiply each element of a matrix by a scalar.\n",
                "\n",
                "* **Example**:\n",
                "  $$\n",
                "  2A = \\begin{bmatrix}2 & 4 \\\\ 6 & 8\\end{bmatrix}\n",
                "  $$\n",
                "* **Application**: Feature scaling or adjusting learning rates."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"2 * A:\\n\", 2 * A)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ”¹ Matrix Multiplication\n",
                "\n",
                "Matrix multiplication involves dot products of rows and columns. For $A$ ($m \\times n$) and $B$ ($n \\times p$), the result is $m \\times p$.\n",
                "\n",
                "* **Example**:\n",
                "  $$\n",
                "  A = \\begin{bmatrix}1 & 2 \\\\ 3 & 4\\end{bmatrix},\\quad B = \\begin{bmatrix}2 & 0 \\\\ 1 & 2\\end{bmatrix}\n",
                "  $$\n",
                "  $$\n",
                "  AB = \\begin{bmatrix}(1*2 + 2*1) & (1*0 + 2*2) \\\\ (3*2 + 4*1) & (3*0 + 4*2)\\end{bmatrix} = \\begin{bmatrix}4 & 4 \\\\ 10 & 8\\end{bmatrix}\n",
                "  $$\n",
                "* **Application**: Neural network layer computations, transformations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"A * B:\\n\", np.dot(A, B))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ”¹ Transpose\n",
                "\n",
                "The transpose flips rows and columns.\n",
                "\n",
                "* **Example**:\n",
                "  $$\n",
                "  A^T = \\begin{bmatrix}1 & 3 \\\\ 2 & 4\\end{bmatrix}\n",
                "  $$\n",
                "* **Application**: Used in covariance matrices or gradient computations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Transpose of A:\\n\", A.T)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "theta = np.pi / 4  # 45 degrees\n",
                "R = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n",
                "v = np.array([1, 0])\n",
                "rotated_v = np.dot(R, v)\n",
                "print(\"Rotated Vector:\", rotated_v)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Systems of Linear Equations\n",
                "\n",
                "Represent systems as $A\\vec{x} = \\vec{b}$.\n",
                "\n",
                "* **Example**:\n",
                "  $$\n",
                "  2x + 3y = 5 \\\\\n",
                "  4x + y = 6\n",
                "  $$\n",
                "  Matrix form:\n",
                "  $$\n",
                "  A = \\begin{bmatrix}2 & 3 \\\\ 4 & 1\\end{bmatrix},\\quad \\vec{x} = \\begin{bmatrix}x \\\\ y\\end{bmatrix},\\quad \\vec{b} = \\begin{bmatrix}5 \\\\ 6\\end{bmatrix}\n",
                "  $$\n",
                "* **Solution**: Solve using inverse or numerical methods.\n",
                "* **Application**: Solving for model parameters in linear regression."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "A = np.array([[2, 3], [4, 1]])\n",
                "b = np.array([5, 6])\n",
                "x = np.linalg.solve(A, b)\n",
                "print(\"Solution (x, y):\", x)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Matrix Inverse and Determinant\n",
                "\n",
                "### ðŸ”¹ Inverse\n",
                "\n",
                "The inverse $A^{-1}$ satisfies $AA^{-1} = I$.\n",
                "\n",
                "* **Formula**:\n",
                "  $$\n",
                "  A = \\begin{bmatrix}a & b \\\\ c & d\\end{bmatrix},\\quad A^{-1} = \\frac{1}{ad - bc} \\begin{bmatrix}d & -b \\\\ -c & a\\end{bmatrix}\n",
                "  $$\n",
                "* **Application**: Solving linear systems, least squares."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "A = np.array([[1, 2], [3, 4]])\n",
                "A_inv = np.linalg.inv(A)\n",
                "print(\"Inverse of A:\\n\", A_inv)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ”¹ Determinant\n",
                "\n",
                "The determinant measures area/volume scaling or invertibility.\n",
                "\n",
                "* **Formula**:\n",
                "  $$\n",
                "  \\text{det}(A) = ad - bc\n",
                "  $$\n",
                "* **Application**: Checking if a matrix is invertible (det â‰  0)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "det_A = np.linalg.det(A)\n",
                "print(\"Determinant of A:\", det_A)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Rank and Linear Independence\n",
                "\n",
                "### ðŸ”¹ Rank\n",
                "\n",
                "The rank is the number of linearly independent rows or columns, indicating the dimension of the column/row space.\n",
                "\n",
                "* **Application**: Determines solvability of linear systems.\n",
                "* **Note**: The matrix $A = \\begin{bmatrix}1 & 2 \\\\ 2 & 4\\end{bmatrix}$ is singular (rank 1), so it is not invertible. This is intentional to demonstrate rank computation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "A = np.array([[1, 2], [2, 4]])\n",
                "rank = np.linalg.matrix_rank(A)\n",
                "print(\"Rank of A:\", rank)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ”¹ Linear Independence\n",
                "\n",
                "Vectors are linearly independent if no non-trivial combination equals zero.\n",
                "\n",
                "* **Example**:\n",
                "  $$\n",
                "  c_1\\vec{v}_1 + c_2\\vec{v}_2 = 0 \\Rightarrow c_1 = c_2 = 0\n",
                "  $$\n",
                "* **Application**: Feature selection in machine learning."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Eigenvalues and Eigenvectors\n",
                "\n",
                "Eigenvectors are vectors that only scale under a transformation: $A\\vec{v} = \\lambda\\vec{v}$.\n",
                "\n",
                "* **Example**:\n",
                "  $$\n",
                "  A = \\begin{bmatrix}2 & 1 \\\\ 1 & 2\\end{bmatrix}\n",
                "  $$\n",
                "  Solve $\\text{det}(A - \\lambda I) = 0$:\n",
                "  $$\n",
                "  \\text{det}\\begin{bmatrix}2-\\lambda & 1 \\\\ 1 & 2-\\lambda\\end{bmatrix} = (2-\\lambda)^2 - 1 = \\lambda^2 - 4\\lambda + 3 = 0\n",
                "  $$\n",
                "  Eigenvalues: $\\lambda = 1, 3$.\n",
                "\n",
                "* **Application**: PCA, stability analysis."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Eigenvalues: [3. 1.]\n",
                        "Eigenvectors:\n",
                        " [[ 0.70710678 -0.70710678]\n",
                        " [ 0.70710678  0.70710678]]\n"
                    ]
                }
            ],
            "source": [
                "A = np.array([[2, 1], [1, 2]])\n",
                "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
                "print(\"Eigenvalues:\", eigenvalues)\n",
                "print(\"Eigenvectors:\\n\", eigenvectors)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Orthogonality and Orthonormality\n",
                "\n",
                "### ðŸ”¹ Orthogonal Vectors\n",
                "\n",
                "Vectors are orthogonal if their dot product is zero.\n",
                "\n",
                "* **Example**: $\\vec{a} \\cdot \\vec{b} = 0$\n",
                "\n",
                "### ðŸ”¹ Orthonormal Vectors\n",
                "\n",
                "Orthogonal vectors with unit norm.\n",
                "\n",
                "* **Application**: Simplifies computations in PCA, QR decomposition."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "v1 = np.array([1, 0])\n",
                "v2 = np.array([0, 1])\n",
                "print(\"Dot Product (Orthogonal):\", np.dot(v1, v2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Projections\n",
                "\n",
                "Projection of $\\vec{a}$ onto $\\vec{b}$ finds the component of $\\vec{a}$ in the direction of $\\vec{b}$.\n",
                "\n",
                "* **Formula**:\n",
                "  $$\n",
                "  \\text{proj}_b a = \\frac{\\vec{a} \\cdot \\vec{b}}{\\vec{b} \\cdot \\vec{b}} \\vec{b}\n",
                "  $$\n",
                "* **Application**: Feature extraction, regression."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "a = np.array([1, 2])\n",
                "b = np.array([1, 0])\n",
                "proj = (np.dot(a, b) / np.dot(b, b)) * b\n",
                "print(\"Projection of a onto b:\", proj)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Norms and Distance\n",
                "\n",
                "### ðŸ”¹ L1 Norm\n",
                "\n",
                "Sum of absolute values.\n",
                "\n",
                "* **Formula**:\n",
                "  $$\n",
                "  \\|\\vec{x}\\|_1 = |x_1| + |x_2| + \\dots + |x_n|\n",
                "  $$\n",
                "* **Application**: Robustness in sparse models.\n",
                "\n",
                "### ðŸ”¹ L2 Norm\n",
                "\n",
                "Euclidean distance.\n",
                "\n",
                "* **Formula**:\n",
                "  $$\n",
                "  \\|\\vec{x}\\|_2 = \\sqrt{x_1^2 + x_2^2 + \\dots + x_n^2}\n",
                "  $$\n",
                "* **Application**: Distance in clustering.\n",
                "\n",
                "### ðŸ”¹ Cosine Similarity\n",
                "\n",
                "Measures angle between vectors.\n",
                "\n",
                "* **Formula**:\n",
                "  $$\n",
                "  \\cos(\\theta) = \\frac{\\vec{a} \\cdot \\vec{b}}{\\|\\vec{a}\\| \\|\\vec{b}\\|}\n",
                "  $$\n",
                "* **Application**: Text similarity in NLP."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "v = np.array([1, 2, 3])\n",
                "print(\"L1 Norm:\", np.linalg.norm(v, 1))\n",
                "print(\"L2 Norm:\", np.linalg.norm(v))\n",
                "cos_sim = np.dot(v, v) / (np.linalg.norm(v) ** 2)\n",
                "print(\"Cosine Similarity (v with itself):\", cos_sim)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Matrix Decompositions\n",
                "\n",
                "### ðŸ”¹ LU Decomposition\n",
                "\n",
                "Decomposes $A = LU$ (L lower triangular, U upper triangular).\n",
                "\n",
                "* **Application**: Solving linear systems efficiently.\n",
                "\n",
                "### ðŸ”¹ QR Decomposition\n",
                "\n",
                "Decomposes $A = QR$ (Q orthogonal, R upper triangular).\n",
                "\n",
                "* **Application**: Least squares, eigenvalue computation.\n",
                "\n",
                "### ðŸ”¹ SVD (Singular Value Decomposition)\n",
                "\n",
                "Decomposes $A = U\\Sigma V^T$, where $U, V$ are orthogonal, $\\Sigma$ is diagonal.\n",
                "\n",
                "* **Application**: PCA, image compression."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "A = np.array([[1, 2], [3, 4]])\n",
                "U, S, Vt = np.linalg.svd(A)\n",
                "print(\"U:\\n\", U)\n",
                "print(\"Singular Values:\", S)\n",
                "print(\"Vt:\\n\", Vt)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Principal Component Analysis (PCA)\n",
                "\n",
                "PCA reduces dimensionality by projecting data onto principal components (eigenvectors of covariance matrix).\n",
                "\n",
                "* **Steps**:\n",
                "  1. Center the data ($X - \\mu$).\n",
                "  2. Compute covariance matrix: $\\Sigma = \\frac{1}{n} X^T X$.\n",
                "  3. Compute eigenvectors/values.\n",
                "  4. Project data onto top eigenvectors.\n",
                "\n",
                "* **Application**: Feature reduction, visualization."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
                "pca = PCA(n_components=1)\n",
                "X_reduced = pca.fit_transform(X)\n",
                "print(\"Reduced Data:\\n\", X_reduced)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 13. Applications in Machine Learning\n",
                "\n",
                "* **Linear Regression**: Solve $\\hat{\\beta} = (X^TX)^{-1}X^Ty$\n",
                "* **PCA**: Dimensionality reduction\n",
                "* **Neural Networks**: Matrix multiplication for layer outputs\n",
                "* **Clustering**: Norms for distance metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X = np.array([[1, 1], [1, 2], [1, 3]])  # Include bias term\n",
                "y = np.array([2, 4, 5])\n",
                "beta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
                "print(\"Coefficients:\", beta)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 14. Basis and Dimension\n",
                "\n",
                "* **Basis**: A set of linearly independent vectors that span a space.\n",
                "* **Dimension**: Number of vectors in the basis.\n",
                "* **Example in $\\mathbb{R}^2$**:\n",
                "  $$\n",
                "  \\left\\{ \\begin{bmatrix}1 \\\\ 0\\end{bmatrix}, \\begin{bmatrix}0 \\\\ 1\\end{bmatrix} \\right\\}\n",
                "  $$\n",
                "* **Application**: Defines the structure of vector spaces in machine learning."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 15. Null Space and Column Space\n",
                "\n",
                "* **Null Space**: Vectors $\\vec{x}$ where $A\\vec{x} = 0$.\n",
                "* **Column Space**: Span of $A$'s columns.\n",
                "* **Application**: Understanding solution sets in linear systems."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "A = np.array([[1, 2], [2, 4]])\n",
                "ns = null_space(A)\n",
                "print(\"Null Space:\\n\", ns)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 16. Change of Basis\n",
                "\n",
                "Express vectors in a new basis using $B^{-1}$.\n",
                "\n",
                "* **Formula**:\n",
                "  $$\n",
                "  [x]_B = B^{-1}x\n",
                "  $$\n",
                "* **Application**: Simplifies computations in PCA."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "B = np.array([[1, 1], [0, 1]])\n",
                "x = np.array([2, 3])\n",
                "x_B = np.linalg.inv(B) @ x\n",
                "print(\"Coordinates in new basis:\", x_B)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 17. Gram-Schmidt Process\n",
                "\n",
                "Orthogonalizes a set of vectors.\n",
                "\n",
                "* **Formula**:\n",
                "  $$\n",
                "  u_1 = v_1, \\quad u_2 = v_2 - \\frac{v_2 \\cdot u_1}{u_1 \\cdot u_1} u_1\n",
                "  $$\n",
                "* **Application**: QR decomposition, orthogonal bases."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def gram_schmidt(V):\n",
                "    U = np.copy(V).astype(float)\n",
                "    for i in range(V.shape[1]):\n",
                "        for j in range(i):\n",
                "            U[:, i] -= np.dot(U[:, i], U[:, j]) / np.dot(U[:, j], U[:, j]) * U[:, j]\n",
                "        U[:, i] /= np.linalg.norm(U[:, i])  # Normalize\n",
                "    return U\n",
                "\n",
                "V = np.array([[1, 1], [0, 1]]).T\n",
                "U = gram_schmidt(V)\n",
                "print(\"Orthonormalized Vectors:\\n\", U)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 18. Positive Definite Matrices\n",
                "\n",
                "A matrix $A$ is positive definite if $\\vec{x}^T A \\vec{x} > 0$ for all $\\vec{x} \\neq 0$.\n",
                "\n",
                "* **Application**: Ensures convergence in optimization (e.g., Hessian in Newtonâ€™s method)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "A = np.array([[2, 1], [1, 2]])\n",
                "eigenvalues = np.linalg.eigvals(A)\n",
                "print(\"Positive Definite (all eigenvalues > 0):\", all(eigenvalues > 0))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 19. Trace of a Matrix\n",
                "\n",
                "Sum of diagonal elements.\n",
                "\n",
                "* **Formula**:\n",
                "  $$\n",
                "  \\text{Tr}(A) = \\sum_i a_{ii}\n",
                "  $$\n",
                "* **Application**: Used in loss functions, optimization."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "A = np.array([[1, 2], [3, 4]])\n",
                "trace = np.trace(A)\n",
                "print(\"Trace of A:\", trace)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 20. Block Matrices\n",
                "\n",
                "Partitioned matrices for efficient computation.\n",
                "\n",
                "* **Example**:\n",
                "  $$\n",
                "  \\begin{bmatrix}A & B \\\\ C & D\\end{bmatrix}\n",
                "  $$\n",
                "* **Application**: Parallel computing, structured data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "A = np.array([[1, 2], [3, 4]])\n",
                "B = np.array([[5, 6], [7, 8]])\n",
                "block_matrix = np.block([[A, B], [B, A]])\n",
                "print(\"Block Matrix:\\n\", block_matrix)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 21. Moore-Penrose Pseudo-Inverse\n",
                "\n",
                "Generalizes inverse for non-square matrices.\n",
                "\n",
                "* **Formula**:\n",
                "  $$\n",
                "  A^+ = (A^TA)^{-1}A^T\n",
                "  $$\n",
                "* **Application**: Least squares solutions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "A = np.array([[1, 2], [3, 4], [5, 6]])\n",
                "A_pinv = np.linalg.pinv(A)\n",
                "print(\"Pseudo-Inverse of A:\\n\", A_pinv)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
